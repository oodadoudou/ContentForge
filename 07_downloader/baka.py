import os
import re
import json
import time
import shutil
import sys
import threading
from pathlib import Path
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import pandas as pd
import concurrent.futures
import requests

# --- è¾…åŠ©å‡½æ•° ---
def load_config(default_url, default_path):
    """ä»JSONæ–‡ä»¶ä¸­åŠ è½½é…ç½®ã€‚"""
    config_file = 'manga_downloader_config.json'
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, KeyError):
            pass
    return {'url': default_url, 'path': default_path}

def save_config(data):
    """å°†é…ç½®ä¿å­˜åˆ°JSONæ–‡ä»¶ã€‚"""
    with open('manga_downloader_config.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4)

def sanitize_for_filename(name):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶åã€‚"""
    if not name: return "Untitled"
    return " ".join(re.sub(r'[<>:"/\\|?*]', '', name).replace(':', ' -').strip().rstrip('. ').split())

def parse_chapter_selection(selection_str, max_chapters):
    """è§£æç”¨æˆ·è¾“å…¥çš„ç« èŠ‚é€‰æ‹© (ä¾‹å¦‚ '1, 3-5, 8')ã€‚"""
    selection_set = set()
    if selection_str.strip().lower() == 'all':
        return list(range(1, max_chapters + 1))
    for part in re.split(r'[,\s]+', selection_str):
        if not part: continue
        try:
            if '-' in part:
                start, end = map(int, part.split('-'))
                selection_set.update(range(min(start, end), max(start, end) + 1))
            else:
                selection_set.add(int(part))
        except ValueError:
            print(f"[!] è­¦å‘Š: æ— æ•ˆè¾“å…¥ '{part}'ã€‚")
    return sorted([num for num in selection_set if 1 <= num <= max_chapters])

def get_timed_input(prompt, timeout=30):
    """å¸¦è¶…æ—¶çš„ç”¨æˆ·è¾“å…¥ã€‚"""
    sys.stdout.write(prompt)
    sys.stdout.flush()
    result = [None]
    def _blocking_input(res):
        try: res[0] = sys.stdin.readline().strip()
        except Exception: pass
    input_thread = threading.Thread(target=_blocking_input, args=(result,))
    input_thread.daemon = True
    input_thread.start()
    input_thread.join(timeout)
    if input_thread.is_alive():
        print()
        return None
    return result[0]

def download_single_image(args):
    """ä½¿ç”¨ requests session å¸¦é‡è¯•åŠŸèƒ½ä¸‹è½½å•ä¸ªå›¾ç‰‡ã€‚"""
    img_url, save_path, session, index, total, chapter_name = args
    if save_path.exists():
        return {'status': 'skipped', 'path': save_path}
    
    for attempt in range(3):
        try:
            response = session.get(img_url, stream=True, timeout=60)
            response.raise_for_status()
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            print(f"    -> (Requests) å·²ä¸‹è½½: {save_path.name} ({index}/{total})")
            return {'status': 'success', 'path': save_path}
        except requests.exceptions.RequestException as e:
            if attempt == 2:
                print(f"    - [Requestså¤±è´¥] {save_path.name}: {e}")
            else:
                time.sleep(1)
    return {'status': 'failed', 'args': args}

# --- ä¸»æµç¨‹ç±» ---
class BakamhPipeline:
    def __init__(self):
        self.driver = None
        self.base_url = ''
        self.failed_downloads = []
        self.successful_chapters = set()
        self.screenshot_successes = []
        self.total_images_downloaded = 0
        self.total_images_skipped = 0

    def _init_driver(self):
        """åˆå§‹åŒ– undetected_chromedriverã€‚"""
        if self.driver: return True
        print("[*] æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨...")
        try:
            options = uc.ChromeOptions()
            # options.add_argument('--headless')
            options.add_argument('--disable-gpu')
            options.add_argument('--no-sandbox')
            options.add_argument('--log-level=3')
            self.driver = uc.Chrome(options=options)
            print("[+] æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–æˆåŠŸã€‚")
            return True
        except Exception as e:
            print(f"[!] æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def get_manga_title(self, url):
        """ä»…è·å–æ¼«ç”»æ ‡é¢˜ç”¨äºæ£€æŸ¥ç¼“å­˜ã€‚"""
        if not self._init_driver(): return None
        print(f"[*] æ­£åœ¨è®¿é—®: {url} ä»¥è·å–æ ‡é¢˜...")
        try:
            self.driver.get(url)
            WebDriverWait(self.driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ol.breadcrumb li:nth-last-child(2) a')))
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            manga_title = sanitize_for_filename(soup.select_one('ol.breadcrumb li:nth-last-child(2) a').text)
            parsed_url = urlparse(url)
            self.base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            print(f"[+] è·å–åˆ°æ¼«ç”»æ ‡é¢˜: {manga_title}")
            return manga_title
        except Exception as e:
            print(f"[!] æ— æ³•ä»åˆå§‹URLè·å–æ¼«ç”»æ ‡é¢˜: {e}")
            return None

    def get_online_chapter_list(self):
        """ä»å½“å‰å·²åŠ è½½çš„é¡µé¢è·å–åŸºç¡€ç« èŠ‚åˆ—è¡¨ã€‚"""
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        online_chapters = []
        seen_urls = set()
        options = soup.select('select.single-chapter-select option')
        options.reverse()
        for option in options:
            if 'data-redirect' in option.attrs:
                chapter_url = option['data-redirect']
                if chapter_url not in seen_urls:
                    online_chapters.append({
                        'name': sanitize_for_filename(option.get_text(strip=True)),
                        'url': chapter_url
                    })
                    seen_urls.add(chapter_url)
        return online_chapters
        
    def scan_chapter_details(self, chapters_to_scan):
        """æ‰«æä¸€ç³»åˆ—ç« èŠ‚ä»¥è·å–å…¶å›¾ç‰‡æ•°é‡ã€‚"""
        scanned_chapters = []
        print(f"[*] å¼€å§‹æ‰«æ {len(chapters_to_scan)} ä¸ªç« èŠ‚çš„è¯¦æƒ…...")
        for i, chapter_info in enumerate(chapters_to_scan):
            print(f"    -> æ­£åœ¨æ‰«æç« èŠ‚ {i+1}/{len(chapters_to_scan)}: {chapter_info['name']}")
            try:
                self.driver.get(chapter_info['url'])
                WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'img.wp-manga-chapter-img')))
                chap_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                img_count = len(chap_soup.select('div.reading-content img.wp-manga-chapter-img'))
                chapter_info['imgs_count'] = img_count
                scanned_chapters.append(chapter_info)
            except Exception as e:
                print(f"      [!] æ‰«æå¤±è´¥ï¼Œè·³è¿‡æ­¤ç« èŠ‚: {e}")
        return scanned_chapters

    def download_with_screenshot(self, args):
        """åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€å›¾ç‰‡å¹¶æˆªå›¾ä¿å­˜ã€‚"""
        img_url, save_path, _, index, total, chapter_name = args
        try:
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            self.driver.get(img_url)
            time.sleep(0.5)
            img_element = self.driver.find_element(By.TAG_NAME, "img")
            img_element.screenshot(str(save_path))
            print(f"    -> (æˆªå›¾) å·²ä¸‹è½½: {save_path.name} ({index}/{total})")
            self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])
            return {'status': 'success'}
        except Exception as e:
            print(f"    - [æˆªå›¾å¤±è´¥] {save_path.name}: {e}")
            if len(self.driver.window_handles) > 1:
                self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])
            return {'status': 'failed', 'url': img_url, 'chapter': chapter_name}

    def process_chapters(self, chapters_to_download, manga_path):
        """è®¿é—®æ¯ä¸ªç« èŠ‚ï¼Œä¼˜å…ˆä½¿ç”¨requestsä¸‹è½½ï¼Œå¤±è´¥åè‡ªåŠ¨åˆ‡æ¢åˆ°æˆªå›¾æ¨¡å¼ã€‚"""
        if not self._init_driver(): return
        
        print("\n--- [é˜¶æ®µä¸‰] å¤„ç†å¹¶ä¸‹è½½ç« èŠ‚ ---")
        original_window = self.driver.current_window_handle

        for i, chapter in enumerate(chapters_to_download):
            chapter_name = f"{chapter['index']:03d} - {chapter['name']}"
            print(f"\n[*] æ­£åœ¨å¤„ç†ç« èŠ‚: {chapter_name} ({i+1}/{len(chapters_to_download)})")
            chapter_output_dir = manga_path / chapter_name
            chapter_output_dir.mkdir(exist_ok=True)
            
            expected_img_count = chapter.get('imgs_count', 0)
            if expected_img_count > 0:
                local_files = [f for f in chapter_output_dir.iterdir() if f.is_file()]
                if len(local_files) >= expected_img_count:
                    print(f"    [*] æ£€æµ‹åˆ°æ‰€æœ‰ {expected_img_count} å¼ å›¾ç‰‡å‡å·²å­˜åœ¨ï¼Œè·³è¿‡æ­¤ç« èŠ‚ã€‚")
                    self.successful_chapters.add(chapter_name)
                    self.total_images_skipped += len(local_files)
                    continue

            page_loaded = False
            for attempt in range(3):
                try:
                    self.driver.get(chapter['url'])
                    WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'img.wp-manga-chapter-img')))
                    page_loaded = True
                    break
                except Exception as e:
                    print(f"    [!] åŠ è½½é¡µé¢å¤±è´¥ (å°è¯• {attempt + 1}/3): {e}")
                    if attempt < 2:
                        print("    [*] 5ç§’åé‡è¯•...")
                        time.sleep(5)
            
            if not page_loaded:
                print(f"    [!] æ— æ³•åŠ è½½ç« èŠ‚é¡µé¢ï¼Œè·³è¿‡æ­¤ç« èŠ‚ã€‚")
                self.failed_downloads.append({'status': 'failed', 'url': chapter['url'], 'chapter': chapter_name})
                continue

            try:
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                browser_cookies = self.driver.get_cookies()
                session = requests.Session()
                for cookie in browser_cookies:
                    session.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])
                
                session.headers.update({
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
                    'Referer': chapter['url']
                })

                image_tags = soup.select('div.reading-content img.wp-manga-chapter-img')
                if not image_tags:
                    print("    [!] è­¦å‘Š: æ­¤é¡µé¢æœªæ‰¾åˆ°å›¾ç‰‡ã€‚")
                    continue

                tasks = []
                for idx, img_tag in enumerate(image_tags):
                    img_src = img_tag.get('src', '').strip()
                    if not img_src: continue
                    img_src = urljoin(self.base_url, img_src)
                    file_ext = os.path.splitext(urlparse(img_src).path)[1] or '.webp'
                    img_filename = f"{idx+1:03d}{file_ext}"
                    save_path = chapter_output_dir / img_filename
                    tasks.append((img_src, save_path, session, idx + 1, len(image_tags), chapter_name))

                print(f"    [*] å‘ç° {len(tasks)} å¼ å›¾ç‰‡ã€‚ä¼˜å…ˆä½¿ç”¨å¹¶è¡Œä¸‹è½½...")
                failed_for_screenshot = []
                with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                    results = executor.map(download_single_image, tasks)
                    for result in results:
                        if result.get('status') == 'failed':
                            failed_for_screenshot.append(result['args'])
                        elif result.get('status') == 'success':
                            self.total_images_downloaded += 1
                        elif result.get('status') == 'skipped':
                            self.total_images_skipped += 1
                
                if failed_for_screenshot:
                    print(f"    [*] {len(failed_for_screenshot)} å¼ å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æˆªå›¾æ¨¡å¼è¿›è¡Œè¡¥æ•‘...")
                    for task_args in failed_for_screenshot:
                        result = self.download_with_screenshot(task_args)
                        if result.get('status') == 'failed':
                            self.failed_downloads.append(result)
                        else:
                            self.total_images_downloaded += 1
                            self.screenshot_successes.append({'path': str(task_args[1]), 'chapter': chapter_name})
                
                if not any(f['chapter'] == chapter_name for f in self.failed_downloads):
                    self.successful_chapters.add(chapter_name)

            except Exception as e:
                print(f"    [!] å¤„ç†ç« èŠ‚ {chapter_name} å¤±è´¥: {e}")
                self.failed_downloads.append({'status': 'failed', 'url': chapter['url'], 'chapter': chapter_name})
            finally:
                self.driver.switch_to.window(original_window)

    def print_summary_report(self, manga_path):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Šï¼ŒåŒ…æ‹¬æˆåŠŸã€å¤±è´¥è¯¦æƒ…å’Œä¿å­˜è·¯å¾„ã€‚"""
        print("\n" + "="*25 + " [æœ€ç»ˆä»»åŠ¡æŠ¥å‘Š] " + "="*25)
        print(f"\n[+] æ¼«ç”»ä¿å­˜ç›®å½•:\n    -> {manga_path.resolve()}")

        total_chapters_processed = len(self.successful_chapters) + len(set(f['chapter'] for f in self.failed_downloads))
        
        if total_chapters_processed == 0:
             print("\n[!] æœªæ‰§è¡Œä»»ä½•ä¸‹è½½ä»»åŠ¡ã€‚")
             print("\n" + "="*68)
             return

        print(f"\n[âœ“] æˆåŠŸå¤„ç† {len(self.successful_chapters)} ä¸ªç« èŠ‚ã€‚")
        print(f"    - æ–°ä¸‹è½½å›¾ç‰‡: {self.total_images_downloaded} å¼ ")
        print(f"    - å·²å­˜åœ¨å¹¶è·³è¿‡: {self.total_images_skipped} å¼ ")

        if self.screenshot_successes:
            print(f"\n  [!] {len(self.screenshot_successes)} å¼ å›¾ç‰‡é€šè¿‡æˆªå›¾æ¨¡å¼è¡¥æ•‘æˆåŠŸ:")
            from itertools import groupby
            sorted_screenshots = sorted(self.screenshot_successes, key=lambda x: x['chapter'])
            for chapter_name, group in groupby(sorted_screenshots, key=lambda x: x['chapter']):
                print(f"      - ç« èŠ‚ [{chapter_name}]:")
                for success in group:
                    print(f"        - {Path(success['path']).name}")
        
        if self.failed_downloads:
            print(f"\n[âœ—] {len(self.failed_downloads)} ä¸ªé¡¹ç›®æœ€ç»ˆä¸‹è½½å¤±è´¥:")
            from itertools import groupby
            sorted_failures = sorted(self.failed_downloads, key=lambda x: x['chapter'])
            for chapter_name, group in groupby(sorted_failures, key=lambda x: x['chapter']):
                print(f"      - ç« èŠ‚ [{chapter_name}]:")
                for failure in group:
                    print(f"        - URL: {failure['url']}")
        elif self.total_images_downloaded > 0 or self.total_images_skipped > 0:
            print("\n[ğŸ‰] æ‰€æœ‰å›¾ç‰‡å‡å·²æˆåŠŸä¸‹è½½ï¼")

        print("\n" + "="*68)

    def close(self):
        """å…³é—­ Selenium WebDriverã€‚"""
        if self.driver:
            try:
                self.driver.quit()
                print("\n[*] æµè§ˆå™¨é©±åŠ¨å·²å…³é—­ã€‚")
            except Exception:
                print("\n[*] æµè§ˆå™¨é©±åŠ¨å¯èƒ½å·²ç»å…³é—­ã€‚")

def main():
    print("--- bakamh.com æ¼«ç”»ä¸‹è½½å™¨ (v2.8 - ç»ˆæç‰ˆ) ---")
    config = load_config("https://bakamh.com/manga/%e6%b7%b1%e6%b8%8a-3/c-1/", "./manga_downloads")
    
    initial_url = input(f"1. è¾“å…¥ä»»æ„ç« èŠ‚URL [{config.get('url')}]: ").strip() or config.get('url')
    output_path = Path(input(f"2. è¾“å…¥ä¸‹è½½æ ¹ç›®å½• [{config.get('path')}]: ").strip() or config.get('path'))

    if not (initial_url and str(output_path)):
        print("[!] URLå’Œè·¯å¾„ä¸èƒ½ä¸ºç©ºã€‚")
        return
    
    save_config({'url': initial_url, 'path': str(output_path)})
    
    pipeline = BakamhPipeline()
    manga_path = None
    try:
        manga_title = pipeline.get_manga_title(initial_url)
        if not manga_title:
            print("[!] æ— æ³•è·å–æ¼«ç”»æ ‡é¢˜ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

        manga_path = output_path / manga_title
        json_path = manga_path / f"{manga_title}_chapters.json"
        manga_path.mkdir(parents=True, exist_ok=True)
        
        chapters = []
        if json_path.exists():
            print(f"[i] å‘ç°æœ¬åœ°ç« èŠ‚ç¼“å­˜æ–‡ä»¶: {json_path}")
            with open(json_path, 'r', encoding='utf-8') as f:
                local_chapters = json.load(f)
            
            print("[*] æ­£åœ¨æ£€æŸ¥ç½‘ç«™æœ‰æ— æ›´æ–°...")
            online_chapters_basic = pipeline.get_online_chapter_list()
            
            if len(online_chapters_basic) <= len(local_chapters):
                print("[+] æœ¬åœ°ç¼“å­˜å·²æ˜¯æœ€æ–°ï¼Œå°†ä»ç¼“å­˜åŠ è½½ã€‚")
                chapters = local_chapters
            else:
                print(f"[!] å‘ç°æ–°ç« èŠ‚ï¼å°†æ›´æ–°æœ¬åœ°ç¼“å­˜ã€‚")
                local_urls = {ch['url'] for ch in local_chapters}
                new_chapters_to_scan = [ch for ch in online_chapters_basic if ch['url'] not in local_urls]
                
                scanned_new_chapters = pipeline.scan_chapter_details(new_chapters_to_scan)
                chapters = local_chapters + scanned_new_chapters
                for i, c in enumerate(chapters):
                    c['index'] = i + 1
                
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(chapters, f, indent=4, ensure_ascii=False)
                print(f"[+] ç« èŠ‚åˆ—è¡¨å·²æ›´æ–°å¹¶ä¿å­˜è‡³: {json_path}")

        else:
            print("[!] æœªå‘ç°æœ¬åœ°ç« èŠ‚ç¼“å­˜ã€‚å°†ä»ç½‘ç«™æ‰«æè·å–ã€‚")
            online_chapters_basic = pipeline.get_online_chapter_list()
            chapters = pipeline.scan_chapter_details(online_chapters_basic)
            if chapters:
                for i, c in enumerate(chapters):
                    c['index'] = i + 1
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(chapters, f, indent=4, ensure_ascii=False)
                print(f"[+] ç« èŠ‚åˆ—è¡¨åŠå›¾ç‰‡æ•°é‡å·²ä¿å­˜è‡³: {json_path}")

        if not chapters:
            print("[!] æ— æ³•è·å–æˆ–åŠ è½½ç« èŠ‚åˆ—è¡¨ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

        print("\n--- [é˜¶æ®µäºŒ] è¯·é€‰æ‹©è¦ä¸‹è½½çš„ç« èŠ‚ ---")
        display_df = pd.DataFrame([{"Index": c['index'], "Chapter Name": c['name'], "Images": c.get('imgs_count', 'N/A')} for c in chapters])
        print(display_df.to_string(index=False))
        
        selection_str = get_timed_input("\nè¾“å…¥ç« èŠ‚åºå· (ä¾‹å¦‚: 1, 3-5, all) [å›è½¦é»˜è®¤ä¸‹è½½å…¨éƒ¨]: ", 30) or 'all'
        selected_indices = parse_chapter_selection(selection_str, len(chapters))
        if not selected_indices:
            print("[!] æœªé€‰æ‹©æœ‰æ•ˆç« èŠ‚ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return
        
        chapters_to_download = [chapters[i-1] for i in selected_indices]
        
        pipeline.process_chapters(chapters_to_download, manga_path)
        
    except Exception as e:
        import traceback
        print(f"\n[!!!] å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        pipeline.close()
        if manga_path:
            pipeline.print_summary_report(manga_path)
        print("\n--- è„šæœ¬æ‰§è¡Œå®Œæ¯• ---")

if __name__ == '__main__':
    main()
