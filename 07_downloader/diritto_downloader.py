import os
import time
import json
import zipfile
import shutil
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- è„šæœ¬æ ¸å¿ƒä»£ç  ---

def load_settings():
    """ä»é¡¹ç›®æ ¹ç›®å½•çš„ shared_assets/settings.json åŠ è½½é…ç½®"""
    settings_path = ""
    try:
        # ä¿®æ­£è·¯å¾„ï¼šä»å½“å‰è„šæœ¬ä½ç½®å‘ä¸Šä¸€çº§ä»¥å®šä½é¡¹ç›®æ ¹ç›®å½•
        script_path = os.path.abspath(__file__)
        # script_dir is .../07_downloader
        script_dir = os.path.dirname(script_path)
        # project_root is .../ContentForge
        project_root = os.path.dirname(script_dir)
        
        settings_path = os.path.join(project_root, 'shared_assets', 'settings.json')
        
        print(f"[ä¿¡æ¯] æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½é…ç½®æ–‡ä»¶: {settings_path}")

        if not os.path.exists(settings_path):
            raise FileNotFoundError

        with open(settings_path, 'r', encoding='utf-8') as f:
            settings = json.load(f)
        return settings
    except FileNotFoundError:
        print(f"âš ï¸ è­¦å‘Š: æœªåœ¨ '{settings_path}' æ‰¾åˆ° settings.json é…ç½®æ–‡ä»¶ã€‚å°†ä½¿ç”¨é»˜è®¤ä¸‹è½½è·¯å¾„ã€‚")
        return {}
    except json.JSONDecodeError:
        print("âŒ é”™è¯¯: settings.json æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ã€‚")
        return {}

def setup_driver():
    """é…ç½®å¹¶è¿æ¥åˆ°å·²ç»æ‰“å¼€çš„ Chrome æµè§ˆå™¨å®ä¾‹"""
    print("æ­£åœ¨å°è¯•è¿æ¥åˆ°å·²å¯åŠ¨çš„ Chrome æµè§ˆå™¨...")
    print("è¯·ç¡®ä¿æ‚¨å·²æŒ‰ç…§è¯´æ˜ä½¿ç”¨ --remote-debugging-port=9222 å¯åŠ¨äº† Chromeã€‚")
    
    options = Options()
    options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
    
    try:
        driver = webdriver.Chrome(options=options)
        print("âœ… æˆåŠŸè¿æ¥åˆ°æµè§ˆå™¨ï¼")
        return driver
    except Exception as e:
        print(f"âŒ è¿æ¥æµè§ˆå™¨å¤±è´¥: {e}")
        print("è¯·ç¡®è®¤ï¼š")
        print("1. Chrome æµè§ˆå™¨æ˜¯å¦å·²é€šè¿‡å‘½ä»¤è¡Œå¯åŠ¨ï¼Œå¹¶å¸¦æœ‰ '--remote-debugging-port=9222' å‚æ•°ï¼Ÿ")
        print("2. æ˜¯å¦æ²¡æœ‰å…¶ä»– Chrome çª—å£ï¼ˆè¯·å®Œå…¨é€€å‡º Chrome åå†æŒ‰æŒ‡ä»¤å¯åŠ¨ï¼‰ï¼Ÿ")
        return None

def scrape_novel(driver, novel_url, download_path):
    """æŠ“å–å°è¯´å†…å®¹å¹¶è¿”å›ç« èŠ‚æ•°æ®"""
    print(f"æ­£åœ¨è®¿é—®å°è¯´ç›®å½•é¡µ: {novel_url}")
    driver.get(novel_url)
    
    try:
        wait = WebDriverWait(driver, 20)
        
        # è·å–å°è¯´æ ‡é¢˜
        novel_title_element = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'p[class*="e1h0h8dc1"]')))
        novel_title = novel_title_element.text.strip().replace('/', '_').replace('\\', '_')
        print(f"ğŸ“˜ å°è¯´æ ‡é¢˜: {novel_title}")

        # ç­‰å¾…ç« èŠ‚åˆ—è¡¨åŠ è½½
        print("æ­£åœ¨è·å–ç« èŠ‚åˆ—è¡¨...")
        chapter_links_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[href*="/episodes/"]')))
        chapter_urls = [elem.get_attribute('href') for elem in chapter_links_elements]
        unique_chapter_urls = sorted(list(set(chapter_urls)), key=chapter_urls.index)
        
        print(f"å…±æ‰¾åˆ° {len(unique_chapter_urls)} ä¸ªç« èŠ‚ã€‚")

        temp_dir = os.path.join(download_path, f"{novel_title}_chapters")
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        os.makedirs(temp_dir)
        print(f"ä¸´æ—¶æ–‡ä»¶å°†ä¿å­˜åœ¨: {temp_dir}")

        for i, url in enumerate(unique_chapter_urls):
            print(f"\n--- æ­£åœ¨æŠ“å–ç¬¬ {i + 1} / {len(unique_chapter_urls)} ç«  ---")
            print(f"URL: {url}")
            driver.get(url)
            
            try:
                chapter_title_element = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'span[class*="e14fx9ai3"]')))
                chapter_title = chapter_title_element.text.strip()

                content_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.tiptap.ProseMirror p')))
                content = "\n\n".join([p.text for p in content_elements if p.text.strip()])
                
                if not content:
                    print("âš ï¸ è­¦å‘Š: æœªèƒ½æŠ“å–åˆ°æœ‰æ•ˆå†…å®¹ï¼Œå¯èƒ½ä¸ºç©ºç™½ç« èŠ‚ã€‚")
                    content = "[æœ¬ç« å†…å®¹ä¸ºç©ºæˆ–æŠ“å–å¤±è´¥]"

                sanitized_title = chapter_title.replace('/', '_').replace('\\', '_').replace(':', 'ï¼š')
                file_name = f"{str(i + 1).zfill(4)}_{sanitized_title}.txt"
                file_path = os.path.join(temp_dir, file_name)
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(f"ç« ç¯€æ¨™é¡Œ: {chapter_title}\n\n")
                    f.write(content)
                
                print(f"âœ… å·²ä¿å­˜: {file_name}")

            except Exception as e:
                print(f"âŒ æŠ“å–æœ¬ç« å¤±è´¥: {e}")
                file_name = f"{str(i + 1).zfill(4)}_æŠ“å–å¤±è´¥.txt"
                file_path = os.path.join(temp_dir, file_name)
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(f"æ— æ³•æŠ“å–æ­¤ç« èŠ‚å†…å®¹ã€‚\nURL: {url}")

            time.sleep(2)

        return novel_title, temp_dir

    except Exception as e:
        print(f"âŒ æŠ“å–å°è¯´æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return None, None

def create_zip(novel_title, source_dir, download_path):
    """å°†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶æ‰“åŒ…æˆ ZIP"""
    zip_filename = os.path.join(download_path, f"{novel_title}.zip")
    print(f"\næ­£åœ¨åˆ›å»º ZIP æ–‡ä»¶: {zip_filename}")
    
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(source_dir):
            for file in sorted(files):
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, source_dir)
                zipf.write(file_path, arcname)
    
    shutil.rmtree(source_dir)
    print(f"âœ… ZIP æ–‡ä»¶åˆ›å»ºæˆåŠŸï¼ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")
    print(f"æ‚¨çš„å°è¯´å·²ä¿å­˜è‡³: {os.path.abspath(zip_filename)}")


if __name__ == "__main__":
    # åŠ è½½é…ç½®
    settings = load_settings()
    # è¯»å– 'default_work_dir' é”®
    default_download_path = settings.get("default_work_dir", os.path.join(os.path.expanduser("~"), "Downloads"))

    # æ‰“å°å‡ºæœ€ç»ˆä½¿ç”¨çš„ä¸‹è½½è·¯å¾„
    print(f"[ä¿¡æ¯] å½“å‰ä¸‹è½½è·¯å¾„è®¾ç½®ä¸º: {default_download_path}")

    # è®©ç”¨æˆ·è¾“å…¥ URL
    novel_url_input = input("è¯·è¾“å…¥Dirittoå°è¯´ç›®å½•é¡µçš„ URL: ").strip()
    if not novel_url_input.startswith("http"):
        print("âŒ é”™è¯¯: æ‚¨è¾“å…¥çš„ URL æ ¼å¼ä¸æ­£ç¡®ã€‚")
    else:
        # ç¡®ä¿ä¸‹è½½ç›®å½•å­˜åœ¨
        if not os.path.exists(default_download_path):
            os.makedirs(default_download_path)
            print(f"å·²åˆ›å»ºä¸‹è½½ç›®å½•: {default_download_path}")

        driver = setup_driver()
        if driver:
            try:
                novel_title, chapters_dir = scrape_novel(driver, novel_url_input, default_download_path)
                if novel_title and chapters_dir:
                    create_zip(novel_title, chapters_dir, default_download_path)
            finally:
                print("\nè„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚æ‚¨å¯ä»¥æ‰‹åŠ¨å…³é—­æµè§ˆå™¨ã€‚")
